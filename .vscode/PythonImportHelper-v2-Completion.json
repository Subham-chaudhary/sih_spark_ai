[
    {
        "label": "runpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "runpy",
        "description": "runpy",
        "detail": "runpy",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "site",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "site",
        "description": "site",
        "detail": "site",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "create_engine",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "create_engine",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "Column",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "Integer",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "String",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "MetaData",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "Table",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "JSON",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "create_engine",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "text",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "sessionmaker",
        "importPath": "sqlalchemy.orm",
        "description": "sqlalchemy.orm",
        "isExtraImport": true,
        "detail": "sqlalchemy.orm",
        "documentation": {}
    },
    {
        "label": "sessionmaker",
        "importPath": "sqlalchemy.orm",
        "description": "sqlalchemy.orm",
        "isExtraImport": true,
        "detail": "sqlalchemy.orm",
        "documentation": {}
    },
    {
        "label": "get_db_main",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "config",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "SessionLocal",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "engine",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "update_config",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "get_db",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "main_engine",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "MainSessionLocal",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "Flask",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "request",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "jsonify",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "text",
        "importPath": "sqlalchemy.sql",
        "description": "sqlalchemy.sql",
        "isExtraImport": true,
        "detail": "sqlalchemy.sql",
        "documentation": {}
    },
    {
        "label": "text",
        "importPath": "sqlalchemy.sql",
        "description": "sqlalchemy.sql",
        "isExtraImport": true,
        "detail": "sqlalchemy.sql",
        "documentation": {}
    },
    {
        "label": "RecursiveCharacterTextSplitter",
        "importPath": "langchain.text_splitter",
        "description": "langchain.text_splitter",
        "isExtraImport": true,
        "detail": "langchain.text_splitter",
        "documentation": {}
    },
    {
        "label": "TextSplitter",
        "importPath": "semantic_text_splitter",
        "description": "semantic_text_splitter",
        "isExtraImport": true,
        "detail": "semantic_text_splitter",
        "documentation": {}
    },
    {
        "label": "bin_dir",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "bin_dir = os.path.dirname(abs_file)\nbase = bin_dir[: -len(\"Scripts\") - 1]  # strip away the bin part from the __file__, plus the path separator\n# prepend bin to PATH (this file is inside the bin directory)\nos.environ[\"PATH\"] = os.pathsep.join([bin_dir, *os.environ.get(\"PATH\", \"\").split(os.pathsep)])\nos.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\nos.environ[\"VIRTUAL_ENV_PROMPT\"] = \"Spark_ai\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "base",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "base = bin_dir[: -len(\"Scripts\") - 1]  # strip away the bin part from the __file__, plus the path separator\n# prepend bin to PATH (this file is inside the bin directory)\nos.environ[\"PATH\"] = os.pathsep.join([bin_dir, *os.environ.get(\"PATH\", \"\").split(os.pathsep)])\nos.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\nos.environ[\"VIRTUAL_ENV_PROMPT\"] = \"Spark_ai\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path)",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "os.environ[\"PATH\"]",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "os.environ[\"PATH\"] = os.pathsep.join([bin_dir, *os.environ.get(\"PATH\", \"\").split(os.pathsep)])\nos.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\nos.environ[\"VIRTUAL_ENV_PROMPT\"] = \"Spark_ai\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "os.environ[\"VIRTUAL_ENV\"]",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "os.environ[\"VIRTUAL_ENV\"] = base  # virtual env is right above bin directory\nos.environ[\"VIRTUAL_ENV_PROMPT\"] = \"Spark_ai\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "os.environ[\"VIRTUAL_ENV_PROMPT\"]",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "os.environ[\"VIRTUAL_ENV_PROMPT\"] = \"Spark_ai\" or os.path.basename(base)  # noqa: SIM222\n# add the virtual environments libraries to the host python import mechanism\nprev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "prev_length",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "prev_length = len(sys.path)\nfor lib in \"..\\\\Lib\\\\site-packages\".split(os.pathsep):\n    path = os.path.realpath(os.path.join(bin_dir, lib))\n    site.addsitedir(path)\nsys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "sys.path[:]",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "sys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]\nsys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "sys.real_prefix",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "sys.real_prefix = sys.prefix\nsys.prefix = base",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "sys.prefix",
        "kind": 5,
        "importPath": ".venv.Scripts.activate_this",
        "description": ".venv.Scripts.activate_this",
        "peekOfCode": "sys.prefix = base",
        "detail": ".venv.Scripts.activate_this",
        "documentation": {}
    },
    {
        "label": "Config",
        "kind": 6,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "class Config:\n    #psql here\n    DB_USER = os.getenv(\"DB_USER\", \"your_db_user\")\n    DB_PASSWORD = os.getenv(\"DB_PASSWORD\", \"your_db_password\")\n    DB_HOST = os.getenv(\"DB_HOST\", \"localhost:5432\")\n    DB_PORT = os.getenv(\"DB_PORT\", \"5432\")\n    DB_NAME = os.getenv(\"DB_NAME\", \"your_db_name\")\n    DATABASE_URL = os.getenv(\"DATABASE_URL\", f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}/{DB_NAME}?sslmode=require&channel_binding=require\")\n    print(\"DATABASE_URL:\", DATABASE_URL)\n    #ollama here",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "update_config",
        "kind": 2,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "def update_config(new_db_url=None, new_db_user=None, new_db_password=None, new_db_host=None, new_db_port=None, new_db_name=None, new_ollama_url=None, new_ollama_model=None, new_main_database_url=None):\n    global config, engine, SessionLocal, main_engine, MainSessionLocal\n    # global config\n    if new_main_database_url:\n        config.MAIN_DATABASE_URL = new_main_database_url\n    if new_db_url:\n        config.DATABASE_URL = new_db_url\n    if new_db_user:\n        config.DB_USER = new_db_user\n    if new_db_password:",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "get_db",
        "kind": 2,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "def get_db():\n    \"\"\"Dependency to get a database session.\"\"\"\n    db = SessionLocal()\n    try:\n        yield db\n    finally:\n        db.close()\ndef get_db_main():\n    db = MainSessionLocal()\n    try:",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "get_db_main",
        "kind": 2,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "def get_db_main():\n    db = MainSessionLocal()\n    try:\n        yield db\n    finally:\n        db.close()",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "config = Config()\nengine = create_engine(config.DATABASE_URL)\nmain_engine = create_engine(config.MAIN_DATABASE_URL)\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\nMainSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=main_engine)\ndef update_config(new_db_url=None, new_db_user=None, new_db_password=None, new_db_host=None, new_db_port=None, new_db_name=None, new_ollama_url=None, new_ollama_model=None, new_main_database_url=None):\n    global config, engine, SessionLocal, main_engine, MainSessionLocal\n    # global config\n    if new_main_database_url:\n        config.MAIN_DATABASE_URL = new_main_database_url",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "engine",
        "kind": 5,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "engine = create_engine(config.DATABASE_URL)\nmain_engine = create_engine(config.MAIN_DATABASE_URL)\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\nMainSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=main_engine)\ndef update_config(new_db_url=None, new_db_user=None, new_db_password=None, new_db_host=None, new_db_port=None, new_db_name=None, new_ollama_url=None, new_ollama_model=None, new_main_database_url=None):\n    global config, engine, SessionLocal, main_engine, MainSessionLocal\n    # global config\n    if new_main_database_url:\n        config.MAIN_DATABASE_URL = new_main_database_url\n    if new_db_url:",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "main_engine",
        "kind": 5,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "main_engine = create_engine(config.MAIN_DATABASE_URL)\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\nMainSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=main_engine)\ndef update_config(new_db_url=None, new_db_user=None, new_db_password=None, new_db_host=None, new_db_port=None, new_db_name=None, new_ollama_url=None, new_ollama_model=None, new_main_database_url=None):\n    global config, engine, SessionLocal, main_engine, MainSessionLocal\n    # global config\n    if new_main_database_url:\n        config.MAIN_DATABASE_URL = new_main_database_url\n    if new_db_url:\n        config.DATABASE_URL = new_db_url",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "SessionLocal",
        "kind": 5,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\nMainSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=main_engine)\ndef update_config(new_db_url=None, new_db_user=None, new_db_password=None, new_db_host=None, new_db_port=None, new_db_name=None, new_ollama_url=None, new_ollama_model=None, new_main_database_url=None):\n    global config, engine, SessionLocal, main_engine, MainSessionLocal\n    # global config\n    if new_main_database_url:\n        config.MAIN_DATABASE_URL = new_main_database_url\n    if new_db_url:\n        config.DATABASE_URL = new_db_url\n    if new_db_user:",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "MainSessionLocal",
        "kind": 5,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "MainSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=main_engine)\ndef update_config(new_db_url=None, new_db_user=None, new_db_password=None, new_db_host=None, new_db_port=None, new_db_name=None, new_ollama_url=None, new_ollama_model=None, new_main_database_url=None):\n    global config, engine, SessionLocal, main_engine, MainSessionLocal\n    # global config\n    if new_main_database_url:\n        config.MAIN_DATABASE_URL = new_main_database_url\n    if new_db_url:\n        config.DATABASE_URL = new_db_url\n    if new_db_user:\n        config.DB_USER = new_db_user",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "get_user_info",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def get_user_info(user_id):\n    try:\n        if not user_id:\n            return \"No user ID provided.\"\n        db_session_main = next(get_db_main()) \n        query = text(\"\"\"\n        SELECT \n        user_name as name, user_role as role, story_titles as program_tile,story_contents as program_content, \n        user_hotspot_locations as location, user_hotspot_names as region, user_hotspot_descriptions as news, \n        watertest_notes as water_test_note, water_qualities as water_quality, waterbody_names as water_body_name, has_global_alert as global_alert, recent_reports as recent_report",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "query_and_embed",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def query_and_embed(query: str):\n    \"\"\"\n    Chunks a query, generates embeddings, performs a similarity search,\n    and returns the content of the most similar medical data entries.\n    \"\"\"\n    try:\n        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n        chunks = text_splitter.split_text(query)\n        if not chunks:\n            return \"No content to process.\"",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "get_config",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def get_config():\n    \"\"\"API endpoint to retrieve application configuration.\"\"\"\n    return jsonify(config.__dict__), 200\n@app.route('/api/config/set', methods=['POST'])\ndef set_config():\n    \"\"\"API endpoint to update application configuration.\"\"\"\n    data = request.get_json()\n    if not data:\n        return jsonify({\"error\": \"Invalid JSON\"}), 400\n    try:",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "set_config",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def set_config():\n    \"\"\"API endpoint to update application configuration.\"\"\"\n    data = request.get_json()\n    if not data:\n        return jsonify({\"error\": \"Invalid JSON\"}), 400\n    try:\n        update_config(\n            new_db_user=data.get('db_user'),\n            new_db_password=data.get('db_password'),\n            new_db_host=data.get('db_host'),",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "process_text",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def process_text():\n    \"\"\"API endpoint to process text, generate embeddings, and store in the database.\"\"\"\n    data = request.get_json()\n    if not data:\n        return jsonify({\"error\": \"empty request\"}), 400\n    try:\n        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n        chunks = text_splitter.split_text(data.get('data'))\n        db_session = next(get_db()) # Get a database session\n        inserted = 0",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "generate_response_endpoint",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def generate_response_endpoint():\n    \"\"\"\n    API endpoint to receive a query, retrieve relevant medical data,\n    and generate a response using the Qwen model.\n    \"\"\"\n    data = request.get_json()\n    if not data or 'query' not in data:\n        return jsonify({\"error\": \"Missing 'query' in request body\"}), 400\n    user_query = data['query']\n    user_id = data['user_id']",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "app = Flask(__name__)\nSYSTEM_PROMPT = \"\"\"\nYou are Spark AI, an advanced medical assistant chatbot. \nYour responsibilities:\n1. Greet the user by their name if available (from user data).\n2. Understand the user’s symptoms or query. If unclear, ask polite clarifying questions.\n3. Use the provided context sources:\n    - Context: retrieved medical knowledge (retrieved_content)\n    - User Data: personal info, region, water quality, alerts, reports\n    to give tailored, empathetic, and concise advice.",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "SYSTEM_PROMPT",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "SYSTEM_PROMPT = \"\"\"\nYou are Spark AI, an advanced medical assistant chatbot. \nYour responsibilities:\n1. Greet the user by their name if available (from user data).\n2. Understand the user’s symptoms or query. If unclear, ask polite clarifying questions.\n3. Use the provided context sources:\n    - Context: retrieved medical knowledge (retrieved_content)\n    - User Data: personal info, region, water quality, alerts, reports\n    to give tailored, empathetic, and concise advice.\n4. Provide only **pre-treatment guidance**: lifestyle tips, self-care, safe over-the-counter options, and awareness of environmental/local risks (e.g., water quality, global alerts).",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "generate_embedding",
        "kind": 2,
        "importPath": "sTs",
        "description": "sTs",
        "peekOfCode": "def generate_embedding(text: str) -> list[float]:\n    try:\n        response = requests.post(OLLAMA_URL, json={\"model\": EMBEDDING_MODEL, \"prompt\": text}, timeout=30)\n        response.raise_for_status()\n        data = response.json()\n        # Ollama embeddings API returns a list of floats in 'embedding' key\n        return data.get('embedding', [])\n    except Exception as e:\n        print(f\"Ollama embedding generation error: {e}\")\n        return [] # Return empty list on error",
        "detail": "sTs",
        "documentation": {}
    },
    {
        "label": "ollama_word_count",
        "kind": 2,
        "importPath": "sTs",
        "description": "sTs",
        "peekOfCode": "def ollama_word_count(text: str) -> int:\n    return len(text.split())\n# Function to fetch data from medicalData2\ndef fetch_medical_data(db_session) -> list[dict]:\n    result = db_session.execute(text(\"SELECT content FROM medicaldata;\")).fetchall()\n    # Convert RowProxy to dictionary\n    print(\"Success: \" ,type(result))\n    print(result[:5][0])\n    return result\nsplitter = TextSplitter.from_callback(ollama_word_count, 250) # Split based on ~200 words",
        "detail": "sTs",
        "documentation": {}
    },
    {
        "label": "fetch_medical_data",
        "kind": 2,
        "importPath": "sTs",
        "description": "sTs",
        "peekOfCode": "def fetch_medical_data(db_session) -> list[dict]:\n    result = db_session.execute(text(\"SELECT content FROM medicaldata;\")).fetchall()\n    # Convert RowProxy to dictionary\n    print(\"Success: \" ,type(result))\n    print(result[:5][0])\n    return result\nsplitter = TextSplitter.from_callback(ollama_word_count, 250) # Split based on ~200 words\n# Function to insert data into medicalData2\ndef insert_chunks(db_session, chunks_with_embeddings):\n    # Assuming we are appending new chunks",
        "detail": "sTs",
        "documentation": {}
    },
    {
        "label": "insert_chunks",
        "kind": 2,
        "importPath": "sTs",
        "description": "sTs",
        "peekOfCode": "def insert_chunks(db_session, chunks_with_embeddings):\n    # Assuming we are appending new chunks\n    insert_statements = []\n    for item in chunks_with_embeddings:\n        insert_statements.append({\n            \"content\": item[\"content\"],\n            \"embedding\": item[\"embedding\"]\n        })\n    if insert_statements:\n        for item in insert_statements:",
        "detail": "sTs",
        "documentation": {}
    },
    {
        "label": "process_data",
        "kind": 2,
        "importPath": "sTs",
        "description": "sTs",
        "peekOfCode": "def process_data():\n    db_session = SessionLocal()\n    try:\n        # Fetch existing data\n        existing_data = fetch_medical_data(db_session)\n        print(f\"Retrieved {len(existing_data)} records from medicalData.\")\n        # Concatenate text from existing data\n        long_text = \"\"\n        for record in existing_data:\n            long_text += record[0]+\" \"",
        "detail": "sTs",
        "documentation": {}
    },
    {
        "label": "DATABASE_URL",
        "kind": 5,
        "importPath": "sTs",
        "description": "sTs",
        "peekOfCode": "DATABASE_URL = os.getenv(\"DATABASE_URL\") \nif not DATABASE_URL:\n    print(\"DATABASE_URL not found in environment variables. Using default value.\")\n    os._exit\nprint(\"DATABASE_URL: \", DATABASE_URL)\n# Ollama API Settings\nOLLAMA_URL = \"http://localhost:11434/api/embeddings\"\nEMBEDDING_MODEL = \"mxbai-embed-large:latest\" # Using the model specified in .env\n# SQLAlchemy Setup\nengine = create_engine(DATABASE_URL)",
        "detail": "sTs",
        "documentation": {}
    },
    {
        "label": "OLLAMA_URL",
        "kind": 5,
        "importPath": "sTs",
        "description": "sTs",
        "peekOfCode": "OLLAMA_URL = \"http://localhost:11434/api/embeddings\"\nEMBEDDING_MODEL = \"mxbai-embed-large:latest\" # Using the model specified in .env\n# SQLAlchemy Setup\nengine = create_engine(DATABASE_URL)\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\nmetadata = MetaData()\n# Function to generate embeddings\ndef generate_embedding(text: str) -> list[float]:\n    try:\n        response = requests.post(OLLAMA_URL, json={\"model\": EMBEDDING_MODEL, \"prompt\": text}, timeout=30)",
        "detail": "sTs",
        "documentation": {}
    },
    {
        "label": "EMBEDDING_MODEL",
        "kind": 5,
        "importPath": "sTs",
        "description": "sTs",
        "peekOfCode": "EMBEDDING_MODEL = \"mxbai-embed-large:latest\" # Using the model specified in .env\n# SQLAlchemy Setup\nengine = create_engine(DATABASE_URL)\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\nmetadata = MetaData()\n# Function to generate embeddings\ndef generate_embedding(text: str) -> list[float]:\n    try:\n        response = requests.post(OLLAMA_URL, json={\"model\": EMBEDDING_MODEL, \"prompt\": text}, timeout=30)\n        response.raise_for_status()",
        "detail": "sTs",
        "documentation": {}
    },
    {
        "label": "engine",
        "kind": 5,
        "importPath": "sTs",
        "description": "sTs",
        "peekOfCode": "engine = create_engine(DATABASE_URL)\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\nmetadata = MetaData()\n# Function to generate embeddings\ndef generate_embedding(text: str) -> list[float]:\n    try:\n        response = requests.post(OLLAMA_URL, json={\"model\": EMBEDDING_MODEL, \"prompt\": text}, timeout=30)\n        response.raise_for_status()\n        data = response.json()\n        # Ollama embeddings API returns a list of floats in 'embedding' key",
        "detail": "sTs",
        "documentation": {}
    },
    {
        "label": "SessionLocal",
        "kind": 5,
        "importPath": "sTs",
        "description": "sTs",
        "peekOfCode": "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\nmetadata = MetaData()\n# Function to generate embeddings\ndef generate_embedding(text: str) -> list[float]:\n    try:\n        response = requests.post(OLLAMA_URL, json={\"model\": EMBEDDING_MODEL, \"prompt\": text}, timeout=30)\n        response.raise_for_status()\n        data = response.json()\n        # Ollama embeddings API returns a list of floats in 'embedding' key\n        return data.get('embedding', [])",
        "detail": "sTs",
        "documentation": {}
    },
    {
        "label": "metadata",
        "kind": 5,
        "importPath": "sTs",
        "description": "sTs",
        "peekOfCode": "metadata = MetaData()\n# Function to generate embeddings\ndef generate_embedding(text: str) -> list[float]:\n    try:\n        response = requests.post(OLLAMA_URL, json={\"model\": EMBEDDING_MODEL, \"prompt\": text}, timeout=30)\n        response.raise_for_status()\n        data = response.json()\n        # Ollama embeddings API returns a list of floats in 'embedding' key\n        return data.get('embedding', [])\n    except Exception as e:",
        "detail": "sTs",
        "documentation": {}
    },
    {
        "label": "splitter",
        "kind": 5,
        "importPath": "sTs",
        "description": "sTs",
        "peekOfCode": "splitter = TextSplitter.from_callback(ollama_word_count, 250) # Split based on ~200 words\n# Function to insert data into medicalData2\ndef insert_chunks(db_session, chunks_with_embeddings):\n    # Assuming we are appending new chunks\n    insert_statements = []\n    for item in chunks_with_embeddings:\n        insert_statements.append({\n            \"content\": item[\"content\"],\n            \"embedding\": item[\"embedding\"]\n        })",
        "detail": "sTs",
        "documentation": {}
    },
    {
        "label": "engine",
        "kind": 5,
        "importPath": "test",
        "description": "test",
        "peekOfCode": "engine = create_engine(\"postgresql://neondb_owner:npg_luKPJEIx8jO6@ep-morning-resonance-adobddox-pooler.c-2.us-east-1.aws.neon.tech/neondb?sslmode=require&channel_binding=require\")\n# 2. Define a query\nquery = text(\"\"\"\n    SELECT \n        user_name as name, user_role as role, story_titles as program_tile,story_contents as program_content, \n        user_hotspot_locations as location, user_hotspot_names as region, user_hotspot_descriptions as news, \n        watertest_notes as water_test_note, water_qualities as water_quality, waterbody_names as water_body_name, has_global_alert as global_alert, recent_reports as recent_report\n    FROM rag_data_view\n    WHERE user_id = :uid\n\"\"\")",
        "detail": "test",
        "documentation": {}
    },
    {
        "label": "query",
        "kind": 5,
        "importPath": "test",
        "description": "test",
        "peekOfCode": "query = text(\"\"\"\n    SELECT \n        user_name as name, user_role as role, story_titles as program_tile,story_contents as program_content, \n        user_hotspot_locations as location, user_hotspot_names as region, user_hotspot_descriptions as news, \n        watertest_notes as water_test_note, water_qualities as water_quality, waterbody_names as water_body_name, has_global_alert as global_alert, recent_reports as recent_report\n    FROM rag_data_view\n    WHERE user_id = :uid\n\"\"\")\n# 3. Execute and fetch results\nwith engine.connect() as conn:",
        "detail": "test",
        "documentation": {}
    }
]